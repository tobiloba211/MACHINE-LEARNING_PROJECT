{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecc9f8",
   "metadata": {},
   "source": [
    "## 1. Introduction to Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d29356",
   "metadata": {},
   "source": [
    "**What is Facial Recognition?**  \n",
    "Facial recognition is a biometric technology used to identify or verify a person’s identity using their face. It analyzes facial features such as the distance between the eyes, nose shape, jawline, and other unique characteristics to match it with a stored facial database. This technology is widely used in various applications, including:\n",
    "- **Security systems** (e.g., surveillance cameras)\n",
    "- **Authentication** (e.g., unlocking phones, access to secure buildings)\n",
    "- **Social media tagging** (e.g., automatic tagging on Facebook)\n",
    "- **Customer service** (e.g., personalized experiences in stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a995d",
   "metadata": {},
   "source": [
    "**How Does Facial Recognition Work?**  \n",
    "- **Step 1: Image Acquisition**: A camera captures an image of the face, which could be a photo or live video.\n",
    "- **Step 2: Face Detection**: The face is detected in the image.\n",
    "- **Step 3: Feature Extraction**: Specific facial features are extracted and measured.\n",
    "- **Step 4: Matching**: These features are compared to faces in a database for identification or verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6941670",
   "metadata": {},
   "source": [
    "### **2. Viola-Jones Algorithm**\n",
    "\n",
    "The **Viola-Jones algorithm** is one of the most popular and efficient methods for real-time object detection, particularly for faces. It was proposed by **Paul Viola** and **Michael Jones** in 2001 and is widely used due to its speed and accuracy.\n",
    "\n",
    "**Key Concepts of Viola-Jones Algorithm**:\n",
    "- **Haar-like features**: These are simple rectangular features that represent changes in intensity between areas of an image. The algorithm uses thousands of these features to detect objects.\n",
    "- **Integral image**: This data structure is used to compute Haar-like features quickly.\n",
    "- **AdaBoost classifier**: It selects the most important features from the large set of Haar features and uses them to create a strong classifier.\n",
    "- **Cascade structure**: The classifier is arranged in stages, where each stage progressively narrows down regions that are likely to contain a face, making it efficient for real-time detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2ab7b",
   "metadata": {},
   "source": [
    "### **3. Haar Cascades in OpenCV**\n",
    "\n",
    "**Haar Cascades** are pre-trained classifiers based on the Viola-Jones algorithm. OpenCV, a popular computer vision library, includes a set of these classifiers trained for face detection, eye detection, smile detection, and more.\n",
    "\n",
    "**How Haar Cascades Work**:\n",
    "- The Haar Cascade classifier works by sliding a window across the image to detect features.\n",
    "- It uses pre-trained data to look for specific patterns that match faces.\n",
    "- Once it identifies a region that matches the Haar-like features of a face, it marks that region as a face.\n",
    "\n",
    "OpenCV provides several pre-trained Haar Cascade classifiers for face detection. One of the most commonly used classifiers is the `haarcascade_frontalface_default.xml` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103d918",
   "metadata": {},
   "source": [
    "### 4. Building a Face Detection App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d5e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python \n",
    "# For computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e44784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python script has been successfully saved to 'face_detection_app_001.py'\n"
     ]
    }
   ],
   "source": [
    "# Code to save the face detection script into a .py file using file handling\n",
    "\n",
    "# Define the content of the .py file as a string\n",
    "code_content = '''\n",
    "\n",
    "# Face Detection using OpenCV and Streamlit \n",
    "# In this section, we will build a simple face detection app using OpenCV to detect faces from the webcam and Streamlit to create an easy-to-use web interface.\n",
    "\n",
    "# Step 1: Install and import Libraries\n",
    "\n",
    "# !pip install opencv-python\n",
    "import cv2\n",
    "import streamlit as st\n",
    "\n",
    "# cv2 (OpenCV): A powerful library for real-time computer vision tasks like face detection.\n",
    "# streamlit: A framework that turns Python scripts into interactive web apps easily.\n",
    "\n",
    "    \n",
    "# Step 2: Load Haar Cascade Classifier\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(r'C:\\\\Users\\\\DELL\\\\Downloads\\\\haarcascade_frontalface_default.xml')\n",
    "# This line loads a pre-trained Haar Cascade classifier for detecting faces. The XML file contains data that helps identify facial features.\n",
    "\n",
    "# Step 3: Detect Faces Function\n",
    "\n",
    "def detect_faces():\n",
    "    cap = cv2.VideoCapture(0)  # Open the default webcam\n",
    "    st.write(\"Press 'q' to stop face detection.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Capture frame-by-frame from webcam\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        # Draw rectangles around detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Show the frame with detected faces\n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        \n",
    "        # Break the loop if 'q' key is pressed\n",
    "        # cv2.waitKey(1) waits for a keypress.\n",
    "        # & 0xFF ensures compatibility with different platforms.\n",
    "        # ord('q') checks if the pressed key is 'q'.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()  # Release the webcam\n",
    "    cv2.destroyAllWindows()  # Close the window\n",
    "    \n",
    "# cv2.VideoCapture(0): Opens the webcam (0 means the default webcam).\n",
    "# cv2.cvtColor(): Converts the captured image to grayscale because the Haar Cascade works better with grayscale images.\n",
    "# detectMultiScale(): Detects faces in the image. It scales the image and looks for potential matches, returning the coordinates of the faces.\n",
    "# cv2.rectangle(): Draws rectangles around the detected faces.\n",
    "\n",
    "    \n",
    "# Step 4: Streamlit Integration\n",
    "\n",
    "def app():\n",
    "    st.title(\"Face Detection using Viola-Jones Algorithm\")\n",
    "    st.write(\"Press the button below to start detecting faces from your webcam.\")\n",
    "    \n",
    "    # Start detecting faces when the button is pressed\n",
    "    if st.button(\"Detect Faces\"):\n",
    "        detect_faces()\n",
    "    \n",
    "    # Add a button to stop face detection\n",
    "    if st.button(\"Stop Detection\"):\n",
    "        st.write(\"Face detection has been stopped.\")  # You can add logic here if needed\n",
    "\n",
    "        \n",
    "# st.title() and st.write() are used to create a title and description on the Streamlit app.\n",
    "# st.button() creates an interactive button that, when clicked, triggers the detect_faces() function.\n",
    "\n",
    "# Step 5: Run the App\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n",
    "\n",
    "'''\n",
    "\n",
    "# Open a file in write mode and save the code content\n",
    "with open('face_detection_app_001.py', 'w') as file:\n",
    "    file.write(code_content)\n",
    "\n",
    "print(\"The Python script has been successfully saved to 'face_detection_app_001.py'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b88d4f",
   "metadata": {},
   "source": [
    "### 5. Here are several possibilities for what you can do after detecting a face (depending on your goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887ee04",
   "metadata": {},
   "source": [
    "#### I. **Display and Save Detected Faces**\n",
    "   - You can extract the region of interest (ROI) containing the face from the image and save it to a file, display it separately, or process it further.\n",
    "\n",
    "```python\n",
    "# After detecting a face, extract and save the face as an image\n",
    "for (x, y, w, h) in faces:\n",
    "    face_roi = frame[y:y+h, x:x+w]  # Extract the region of interest (the face)\n",
    "    cv2.imwrite('detected_face.jpg', face_roi)  # Save the face image to a file\n",
    "```\n",
    "\n",
    "This saves the detected face as a new image file (`detected_face.jpg`), which can be useful for further processing, such as identification, recognition, or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ceb1b3",
   "metadata": {},
   "source": [
    "#### II. **Face Recognition**\n",
    "   - If your goal is to **recognize** the face (i.e., determine whose face it is), you'll need to train a model on labeled faces. After detecting the face, you could pass it into a face recognition system using methods like:\n",
    "     - **Deep Learning-based Recognition**: Using libraries like `face_recognition` (which is built on dlib) or OpenCV’s DNN modules.\n",
    "     - **Pre-trained Models**: For example, you can use pre-trained face recognition models (e.g., `face_recognition` Python library).\n",
    "\n",
    "##### Example using `face_recognition`:\n",
    "```python\n",
    "import face_recognition\n",
    "\n",
    "# Assuming you have detected the face and extracted the ROI (face_roi)\n",
    "face_encodings = face_recognition.face_encodings(face_roi)\n",
    "if face_encodings:\n",
    "    known_face_encodings = [known_face_encoding1, known_face_encoding2]  # List of known faces\n",
    "    matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "\n",
    "    if True in matches:\n",
    "        first_match_index = matches.index(True)\n",
    "        name = known_face_names[first_match_index]  # Name associated with the detected face\n",
    "        print(f\"Face recognized: {name}\")\n",
    "    else:\n",
    "        print(\"Unknown face detected\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c18cd0",
   "metadata": {},
   "source": [
    "#### III. **Emotion Detection or Expression Analysis**\n",
    "   - You can use pre-trained models to detect facial expressions (e.g., happy, sad, angry). There are libraries like `FER` (Facial Expression Recognition) that make this process easier.\n",
    "   \n",
    "```python\n",
    "from fer import FER\n",
    "\n",
    "# Use a pre-trained emotion detection model\n",
    "emotion_detector = FER()\n",
    "emotions = emotion_detector.detect_emotions(face_roi)\n",
    "\n",
    "if emotions:\n",
    "    print(\"Detected emotions:\", emotions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35a415",
   "metadata": {},
   "source": [
    "#### IV. **Facial Feature Detection**\n",
    "   - After detecting a face, you might want to detect specific facial landmarks (e.g., eyes, nose, mouth). You can use libraries like `dlib` or OpenCV’s pre-trained landmark detectors.\n",
    "\n",
    "```python\n",
    "# Assuming you detected a face (in gray-scale)\n",
    "landmarks = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "for (x, y, w, h) in landmarks:\n",
    "    # Extract the region of interest for eyes or other facial landmarks\n",
    "    eyes = cv2.CascadeClassifier('haarcascade_eye.xml').detectMultiScale(gray[y:y+h, x:x+w])\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        cv2.rectangle(frame, (x + ex, y + ey), (x + ex + ew, y + ey + eh), (0, 255, 0), 2)  # Draw rectangle around the eyes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02355d",
   "metadata": {},
   "source": [
    "#### V. **Face Tracking**\n",
    "   - Instead of just detecting the face once, you could track it in real-time, even if the face moves in the video feed. This can be done using tracking algorithms like KLT (Kanade-Lucas-Tomasi), CSRT, or MedianFlow available in OpenCV.\n",
    "\n",
    "```python\n",
    "tracker = cv2.TrackerCSRT_create()  # Create a tracker object\n",
    "tracker.init(frame, (x, y, w, h))  # Initialize tracker with detected face position\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Capture frame\n",
    "    success, box = tracker.update(frame)  # Update the tracker with the new frame\n",
    "    if success:\n",
    "        p1 = (int(box[0]), int(box[1]))\n",
    "        p2 = (int(box[0] + box[2]), int(box[1] + box[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (255, 0, 0), 2)  # Draw rectangle around the tracked face\n",
    "    cv2.imshow('Face Tracking', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd170f1b",
   "metadata": {},
   "source": [
    "#### VI. **Blur or Obfuscate Faces (Privacy Concerns)**\n",
    "   - You can blur or pixelate detected faces for privacy reasons, especially in public surveillance or data protection use cases.\n",
    "\n",
    "```python\n",
    "for (x, y, w, h) in faces:\n",
    "    face = frame[y:y+h, x:x+w]\n",
    "    blurred_face = cv2.GaussianBlur(face, (99, 99), 30)  # Apply Gaussian blur to the face\n",
    "    frame[y:y+h, x:x+w] = blurred_face  # Replace the original face with the blurred one\n",
    "cv2.imshow('Blurred Faces', frame)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e4ab8",
   "metadata": {},
   "source": [
    "#### VII. **Real-Time Filters (Fun Applications)**\n",
    "   - You can apply fun filters to the detected face, like Snapchat-style filters (e.g., adding glasses, hats, etc.). This involves overlaying custom images (filters) on top of the detected face.\n",
    "\n",
    "```python\n",
    "# Load a filter image (e.g., sunglasses)\n",
    "filter_img = cv2.imread('sunglasses.png', -1)\n",
    "\n",
    "# After detecting a face, place the filter on the face\n",
    "for (x, y, w, h) in faces:\n",
    "    filter_img_resized = cv2.resize(filter_img, (w, h))  # Resize the filter to fit the face\n",
    "    frame[y:y+h, x:x+w] = apply_filter(frame[y:y+h, x:x+w], filter_img_resized)  # Function to overlay filter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0ede9",
   "metadata": {},
   "source": [
    "#### VIII. **Facial Attribute Analysis**\n",
    "   - You can analyze detected faces for attributes like gender, age, and ethnicity using pre-trained deep learning models available in OpenCV or other libraries.\n",
    "\n",
    "```python\n",
    "# Load a pre-trained model for age and gender prediction\n",
    "age_net = cv2.dnn.readNetFromCaffe('deploy_age.prototxt', 'age_net.caffemodel')\n",
    "gender_net = cv2.dnn.readNetFromCaffe('deploy_gender.prototxt', 'gender_net.caffemodel')\n",
    "\n",
    "# After detecting a face, pass it to the model\n",
    "blob = cv2.dnn.blobFromImage(face_roi, 1.0, (227, 227), (104, 117, 123), swapRB=False)\n",
    "gender_net.setInput(blob)\n",
    "gender = gender_net.forward()  # Predict gender\n",
    "\n",
    "age_net.setInput(blob)\n",
    "age = age_net.forward()  # Predict age\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e7bd5",
   "metadata": {},
   "source": [
    "### Let's enhance the initial face detection app with the ff features:\n",
    "\n",
    "1. **Instructions in the Streamlit app interface**: Guide the user on how to use the app.\n",
    "2. **Save the detected faces**: Allow the user to save images with detected faces on their device.\n",
    "3. **Change rectangle color**: Allow the user to pick the color of the rectangles drawn around the detected faces.\n",
    "4. **Adjust `minNeighbors` parameter**: Let the user control this parameter for tuning detection sensitivity.\n",
    "5. **Adjust `scaleFactor` parameter**: Let the user control this parameter for tuning how the image scales during detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da15ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python script has been successfully saved to 'face_detection_app_2.py'\n"
     ]
    }
   ],
   "source": [
    "code_content = '''\n",
    "\n",
    "# Import required libraries\n",
    "import cv2  # OpenCV for real-time computer vision tasks\n",
    "import streamlit as st  # Streamlit for web app interface\n",
    "from PIL import Image  # For handling image operations\n",
    "import numpy as np  # For numerical computations\n",
    "import os  # For interacting with the operating system\n",
    "import time\n",
    "\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(r'C:\\\\Users\\\\DELL\\\\Downloads\\\\haarcascade_frontalface_default.xml')\n",
    "# The XML file contains the pre-trained model for detecting faces.\n",
    "\n",
    "# Step 1: Function to detect faces from webcam feed\n",
    "def detect_faces(scaleFactor, minNeighbors, color_choice):\n",
    "    cap = cv2.VideoCapture(0)  # Opens the default webcam (0 refers to the default device)\n",
    "    stframe = st.empty()  # Placeholder for displaying the webcam video in Streamlit\n",
    "    saved = False  # Initialize a saved flag for the save button functionality\n",
    "\n",
    "    # Check if stop detection flag exists in session state, initialize if not\n",
    "    if \"stop_detection\" not in st.session_state:\n",
    "        st.session_state.stop_detection = False\n",
    "\n",
    "    # Checkbox to stop face detection\n",
    "    stop_detection = st.checkbox(\"Stop Face Detection\", key=\"stop_detection_checkbox\")\n",
    "\n",
    "    while not st.session_state.stop_detection:  # Loop until stop detection is triggered\n",
    "        ret, frame = cap.read()  # Capture each frame\n",
    "        if not ret:  # If the frame is not captured successfully\n",
    "            st.error(\"Failed to capture image from webcam.\")  # Display an error message\n",
    "            break\n",
    "        \n",
    "        # Convert the frame to grayscale (face detection works better on grayscale images)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces in the grayscale image\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=(30, 30))\n",
    "        \n",
    "        # Convert the selected color from hex format (RGB) to BGR for OpenCV\n",
    "        color_rgb = tuple(int(color_choice.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "        color_bgr = (color_rgb[2], color_rgb[1], color_rgb[0])  # Reverse the RGB to BGR\n",
    "\n",
    "        \n",
    "        # Draw rectangles around detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color_bgr, 2)  # Draw rectangle on each detected face\n",
    "        \n",
    "        # Display the current frame with detected faces in the Streamlit app\n",
    "        stframe.image(frame, channels=\"BGR\", use_column_width=True)\n",
    "        \n",
    "        # Create a unique key for each save button using the current timestamp\n",
    "        button_key = f\"save_image_{str(time.time())}\"\n",
    "        \n",
    "        # Button to save the image with detected faces\n",
    "        if st.button(\"Save Image with Detected Faces\", key=button_key) and not saved:\n",
    "            cv2.imwrite(\"detected_faces.png\", frame)  # Save the current frame as an image\n",
    "            st.success(\"Image saved as detected_faces.png\")  # Display success message\n",
    "            saved = True  # Set the flag to prevent multiple saves\n",
    "        \n",
    "        # Update the stop detection flag if the checkbox is checked\n",
    "        if stop_detection:\n",
    "            st.session_state.stop_detection = True\n",
    "\n",
    "    cap.release()  # Release the webcam resource\n",
    "    cv2.destroyAllWindows()  # Close any OpenCV windows\n",
    "\n",
    "# Step 2: Define the Streamlit app interface\n",
    "def app():\n",
    "    st.title(\"Face Detection App using Viola-Jones Algorithm\")  # Title of the app\n",
    "    \n",
    "    # Instructions for the user\n",
    "    st.markdown(\"\"\"\n",
    "    ### Instructions:\n",
    "    1. Use the **slider** below to adjust the `scaleFactor` and `minNeighbors` parameters for face detection.\n",
    "    2. Choose the **color** of the rectangle that will be drawn around the detected faces.\n",
    "    3. Press the **'Detect Faces'** button to start detecting faces using your webcam.\n",
    "    4. Optionally, save the detected face image by pressing the **'Save Image with Detected Faces'** button.\n",
    "    5. Use the checkbox to **stop detection**.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Slider for adjusting the `scaleFactor` (resizing of the image for detection)\n",
    "    scaleFactor = st.slider(\"Adjust scaleFactor (Image resizing)\", min_value=1.01, max_value=2.0, value=1.1, step=0.01)\n",
    "    \n",
    "    # Slider for adjusting `minNeighbors` (controls detection sensitivity)\n",
    "    minNeighbors = st.slider(\"Adjust minNeighbors (Detection sensitivity)\", min_value=3, max_value=10, value=5, step=1)\n",
    "    \n",
    "    # Color picker for the rectangle that will highlight detected faces\n",
    "    color_choice = st.color_picker(\"Pick a color for the detection rectangle\", \"#FF0000\")\n",
    "    \n",
    "    # Button to start face detection\n",
    "    if st.button(\"Detect Faces\", key=\"detect_faces\"):\n",
    "        detect_faces(scaleFactor, minNeighbors, color_choice)  # Call the face detection function with parameters\n",
    "\n",
    "# Step 3: Run the Streamlit app\n",
    "if __name__ == \"__main__\":  # If the script is run directly\n",
    "    app()  # Run the app\n",
    "    \n",
    "'''\n",
    "\n",
    "# Open a file in write mode and save the code content\n",
    "with open('face_detection_app_2.py', 'w') as file:\n",
    "    file.write(code_content)\n",
    "\n",
    "print(\"The Python script has been successfully saved to 'face_detection_app_2.py'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61c72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
